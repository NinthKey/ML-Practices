{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network\n",
    "## 1. Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(128, 128,..., activation=\"relu\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\huang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \n",
      "C:\\Users\\huang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  \n",
      "C:\\Users\\huang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\huang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Initializing CNN\n",
    "# Building two layers of Convolution layer, pooling, and two layers of neural network\n",
    "classifier = Sequential() \n",
    "classifier.add(Conv2D(32,3,3, input_shape = (128, 128, 3),activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(32,3,3, activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(32,3,3, activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(output_dim = 128, activation = \"relu\"))\n",
    "classifier.add(Dense(output_dim = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Fitting the CNN to the images (Copy from Keras documentation)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (128, 128),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (128, 128),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=8000, epochs=90, validation_data=<keras.pre..., validation_steps=2000, workers=12, max_queue_size=100)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "8000/8000 [==============================] - 230s 29ms/step - loss: 0.3184 - acc: 0.8522 - val_loss: 0.5273 - val_acc: 0.8466\n",
      "Epoch 2/90\n",
      "8000/8000 [==============================] - 225s 28ms/step - loss: 0.0790 - acc: 0.9707 - val_loss: 0.7629 - val_acc: 0.8469\n",
      "Epoch 3/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0417 - acc: 0.9855 - val_loss: 0.9558 - val_acc: 0.8462\n",
      "Epoch 4/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0314 - acc: 0.9894 - val_loss: 1.0079 - val_acc: 0.8544\n",
      "Epoch 5/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0251 - acc: 0.9916 - val_loss: 0.9754 - val_acc: 0.8466\n",
      "Epoch 6/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0212 - acc: 0.9931 - val_loss: 1.0553 - val_acc: 0.8516\n",
      "Epoch 7/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0195 - acc: 0.9938 - val_loss: 1.0709 - val_acc: 0.8505\n",
      "Epoch 8/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0168 - acc: 0.9945 - val_loss: 1.1049 - val_acc: 0.8540\n",
      "Epoch 9/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0158 - acc: 0.9951 - val_loss: 1.0348 - val_acc: 0.8589\n",
      "Epoch 10/90\n",
      "8000/8000 [==============================] - 223s 28ms/step - loss: 0.0143 - acc: 0.9956 - val_loss: 1.1858 - val_acc: 0.8406\n",
      "Epoch 11/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0122 - acc: 0.9961 - val_loss: 1.0435 - val_acc: 0.8636\n",
      "Epoch 12/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0126 - acc: 0.9961 - val_loss: 1.0557 - val_acc: 0.8601\n",
      "Epoch 13/90\n",
      "8000/8000 [==============================] - 226s 28ms/step - loss: 0.0121 - acc: 0.9964 - val_loss: 1.0628 - val_acc: 0.8625\n",
      "Epoch 14/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0104 - acc: 0.9968 - val_loss: 1.2178 - val_acc: 0.8580\n",
      "Epoch 15/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0110 - acc: 0.9966 - val_loss: 1.0732 - val_acc: 0.8581\n",
      "Epoch 16/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0099 - acc: 0.9970 - val_loss: 1.1372 - val_acc: 0.8583\n",
      "Epoch 17/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0106 - acc: 0.9969 - val_loss: 1.2151 - val_acc: 0.8460\n",
      "Epoch 18/90\n",
      "8000/8000 [==============================] - 225s 28ms/step - loss: 0.0097 - acc: 0.9971 - val_loss: 1.1460 - val_acc: 0.8601\n",
      "Epoch 19/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0099 - acc: 0.9972 - val_loss: 1.2761 - val_acc: 0.8591\n",
      "Epoch 20/90\n",
      "8000/8000 [==============================] - 225s 28ms/step - loss: 0.0106 - acc: 0.9971 - val_loss: 1.2396 - val_acc: 0.8525\n",
      "Epoch 21/90\n",
      "8000/8000 [==============================] - 225s 28ms/step - loss: 0.0092 - acc: 0.9975 - val_loss: 1.5491 - val_acc: 0.8379\n",
      "Epoch 22/90\n",
      "8000/8000 [==============================] - 224s 28ms/step - loss: 0.0101 - acc: 0.9973 - val_loss: 1.3412 - val_acc: 0.8600\n",
      "Epoch 23/90\n",
      "8000/8000 [==============================] - 225s 28ms/step - loss: 0.0101 - acc: 0.9973 - val_loss: 1.2519 - val_acc: 0.8667\n",
      "Epoch 24/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0099 - acc: 0.9975 - val_loss: 1.3194 - val_acc: 0.8586\n",
      "Epoch 25/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0099 - acc: 0.9975 - val_loss: 1.3560 - val_acc: 0.8555\n",
      "Epoch 26/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0107 - acc: 0.9976 - val_loss: 1.2288 - val_acc: 0.8658\n",
      "Epoch 27/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0104 - acc: 0.9975 - val_loss: 1.2001 - val_acc: 0.8706\n",
      "Epoch 28/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0094 - acc: 0.9977 - val_loss: 1.2118 - val_acc: 0.8679\n",
      "Epoch 29/90\n",
      "8000/8000 [==============================] - 230s 29ms/step - loss: 0.0089 - acc: 0.9977 - val_loss: 1.3105 - val_acc: 0.8630\n",
      "Epoch 30/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0104 - acc: 0.9975 - val_loss: 1.3328 - val_acc: 0.8637\n",
      "Epoch 31/90\n",
      "8000/8000 [==============================] - 244s 30ms/step - loss: 0.0104 - acc: 0.9977 - val_loss: 1.3695 - val_acc: 0.8630\n",
      "Epoch 32/90\n",
      "8000/8000 [==============================] - 256s 32ms/step - loss: 0.0101 - acc: 0.9977 - val_loss: 1.4972 - val_acc: 0.8565\n",
      "Epoch 33/90\n",
      "8000/8000 [==============================] - 269s 34ms/step - loss: 0.0102 - acc: 0.9976 - val_loss: 1.4574 - val_acc: 0.8627\n",
      "Epoch 34/90\n",
      "8000/8000 [==============================] - 269s 34ms/step - loss: 0.0101 - acc: 0.9977 - val_loss: 1.4482 - val_acc: 0.8575\n",
      "Epoch 35/90\n",
      "8000/8000 [==============================] - 270s 34ms/step - loss: 0.0099 - acc: 0.9978 - val_loss: 1.5577 - val_acc: 0.8549\n",
      "Epoch 36/90\n",
      "8000/8000 [==============================] - 264s 33ms/step - loss: 0.0100 - acc: 0.9979 - val_loss: 1.5589 - val_acc: 0.8580\n",
      "Epoch 37/90\n",
      "8000/8000 [==============================] - 233s 29ms/step - loss: 0.0112 - acc: 0.9976 - val_loss: 1.5047 - val_acc: 0.8561\n",
      "Epoch 38/90\n",
      "8000/8000 [==============================] - 235s 29ms/step - loss: 0.0109 - acc: 0.9976 - val_loss: 1.5918 - val_acc: 0.8599\n",
      "Epoch 39/90\n",
      "8000/8000 [==============================] - 230s 29ms/step - loss: 0.0111 - acc: 0.9979 - val_loss: 1.8103 - val_acc: 0.8446\n",
      "Epoch 40/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0111 - acc: 0.9977 - val_loss: 1.4545 - val_acc: 0.8650\n",
      "Epoch 41/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0110 - acc: 0.9979 - val_loss: 1.5741 - val_acc: 0.8595\n",
      "Epoch 42/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0127 - acc: 0.9977 - val_loss: 1.6286 - val_acc: 0.8572\n",
      "Epoch 43/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0134 - acc: 0.9976 - val_loss: 1.6457 - val_acc: 0.8580\n",
      "Epoch 44/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0124 - acc: 0.9977 - val_loss: 1.6177 - val_acc: 0.8531\n",
      "Epoch 45/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0124 - acc: 0.9978 - val_loss: 1.6092 - val_acc: 0.8615\n",
      "Epoch 46/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0144 - acc: 0.9977 - val_loss: 1.6366 - val_acc: 0.8670\n",
      "Epoch 47/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0132 - acc: 0.9978 - val_loss: 2.1016 - val_acc: 0.8442\n",
      "Epoch 48/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0129 - acc: 0.9979 - val_loss: 1.7932 - val_acc: 0.8470\n",
      "Epoch 49/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0126 - acc: 0.9980 - val_loss: 1.7678 - val_acc: 0.8538\n",
      "Epoch 50/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0138 - acc: 0.9978 - val_loss: 1.7073 - val_acc: 0.8601\n",
      "Epoch 51/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0148 - acc: 0.9978 - val_loss: 1.7644 - val_acc: 0.8656\n",
      "Epoch 52/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0142 - acc: 0.9980 - val_loss: 1.8134 - val_acc: 0.8564\n",
      "Epoch 53/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0155 - acc: 0.9978 - val_loss: 1.7700 - val_acc: 0.8580\n",
      "Epoch 54/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0157 - acc: 0.9979 - val_loss: 1.8499 - val_acc: 0.8667\n",
      "Epoch 55/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0169 - acc: 0.9978 - val_loss: 1.9489 - val_acc: 0.8522\n",
      "Epoch 56/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0171 - acc: 0.9978 - val_loss: 1.7936 - val_acc: 0.8646\n",
      "Epoch 57/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0189 - acc: 0.9977 - val_loss: 1.8709 - val_acc: 0.8620\n",
      "Epoch 58/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0176 - acc: 0.9978 - val_loss: 1.7967 - val_acc: 0.8650\n",
      "Epoch 59/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0185 - acc: 0.9979 - val_loss: 2.0752 - val_acc: 0.8480\n",
      "Epoch 60/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 226s 28ms/step - loss: 0.0221 - acc: 0.9976 - val_loss: 1.8395 - val_acc: 0.8650\n",
      "Epoch 61/90\n",
      "8000/8000 [==============================] - 226s 28ms/step - loss: 0.0265 - acc: 0.9974 - val_loss: 1.8717 - val_acc: 0.8644\n",
      "Epoch 62/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0265 - acc: 0.9975 - val_loss: 1.9640 - val_acc: 0.8630\n",
      "Epoch 63/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0265 - acc: 0.9975 - val_loss: 2.0824 - val_acc: 0.8540\n",
      "Epoch 64/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0314 - acc: 0.9972 - val_loss: 2.0298 - val_acc: 0.8616\n",
      "Epoch 65/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0316 - acc: 0.9973 - val_loss: 1.9345 - val_acc: 0.8648\n",
      "Epoch 66/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0321 - acc: 0.9972 - val_loss: 2.2097 - val_acc: 0.8516\n",
      "Epoch 67/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0342 - acc: 0.9971 - val_loss: 2.2078 - val_acc: 0.8532\n",
      "Epoch 68/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0369 - acc: 0.9970 - val_loss: 1.9875 - val_acc: 0.8645\n",
      "Epoch 69/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0368 - acc: 0.9971 - val_loss: 1.9472 - val_acc: 0.8730\n",
      "Epoch 70/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0386 - acc: 0.9970 - val_loss: 2.0846 - val_acc: 0.8589\n",
      "Epoch 71/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0462 - acc: 0.9965 - val_loss: 2.1459 - val_acc: 0.8600\n",
      "Epoch 72/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0515 - acc: 0.9962 - val_loss: 2.0729 - val_acc: 0.8674\n",
      "Epoch 73/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0493 - acc: 0.9964 - val_loss: 2.2612 - val_acc: 0.8538\n",
      "Epoch 74/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0583 - acc: 0.9958 - val_loss: 2.2845 - val_acc: 0.8521\n",
      "Epoch 75/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0657 - acc: 0.9954 - val_loss: 2.2532 - val_acc: 0.8541\n",
      "Epoch 76/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.0685 - acc: 0.9952 - val_loss: 2.3087 - val_acc: 0.8524\n",
      "Epoch 77/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0746 - acc: 0.9948 - val_loss: 1.9893 - val_acc: 0.8711\n",
      "Epoch 78/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0680 - acc: 0.9953 - val_loss: 2.1597 - val_acc: 0.8596\n",
      "Epoch 79/90\n",
      "8000/8000 [==============================] - 227s 28ms/step - loss: 0.0831 - acc: 0.9944 - val_loss: 2.2111 - val_acc: 0.8601\n",
      "Epoch 80/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0934 - acc: 0.9937 - val_loss: 2.2693 - val_acc: 0.8566\n",
      "Epoch 81/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.0838 - acc: 0.9944 - val_loss: 2.2093 - val_acc: 0.8595\n",
      "Epoch 82/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.1158 - acc: 0.9923 - val_loss: 2.1684 - val_acc: 0.8625\n",
      "Epoch 83/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0999 - acc: 0.9934 - val_loss: 2.2822 - val_acc: 0.8552\n",
      "Epoch 84/90\n",
      "8000/8000 [==============================] - 230s 29ms/step - loss: 0.1178 - acc: 0.9923 - val_loss: 2.2289 - val_acc: 0.8566\n",
      "Epoch 85/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0883 - acc: 0.9941 - val_loss: 2.4884 - val_acc: 0.8415\n",
      "Epoch 86/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.0994 - acc: 0.9935 - val_loss: 2.5490 - val_acc: 0.8379\n",
      "Epoch 87/90\n",
      "8000/8000 [==============================] - 228s 28ms/step - loss: 0.1130 - acc: 0.9926 - val_loss: 2.2691 - val_acc: 0.8565\n",
      "Epoch 88/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.1342 - acc: 0.9912 - val_loss: 2.2747 - val_acc: 0.8561\n",
      "Epoch 89/90\n",
      "8000/8000 [==============================] - 229s 29ms/step - loss: 0.1426 - acc: 0.9908 - val_loss: 2.3911 - val_acc: 0.8479\n",
      "Epoch 90/90\n",
      "8000/8000 [==============================] - 228s 29ms/step - loss: 0.1424 - acc: 0.9908 - val_loss: 2.4163 - val_acc: 0.8486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1adafab00f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train CNN\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 90,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000, \n",
    "                         workers=12,\n",
    "                         max_q_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "Using Keras to build CNN is very easy and saves a lot of time. It is easy to understand what happen on the top level with this framework. Hopefully I'll get to understand more in details as I continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
